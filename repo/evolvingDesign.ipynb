{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 115 ms, sys: 31.7 ms, total: 147 ms\n",
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## de  ultracom/compensar/tasks/constants.py \n",
    "\n",
    "cte_PRIMARY_KEYS = {\n",
    "            'flowConnection': [\"name\"],\n",
    "            'flowVariable': [\"nameSpace\", \"name\"],\n",
    "            'flowParameter': [\"nameSpace\", \"parameterName\"],\n",
    "            'flowTask': [\"name\"],\n",
    "            'taskOutput': [\"key\"],\n",
    "            'connectionType': [\"type\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb\n",
      "response_1600883647838.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_1600883647838.json') as f:\n",
    "    flowInfo = json.load(f)\n",
    "\n",
    "#keys = [el for el in list(flowInfo.keys()) if type(flowInfo[el]) is list]\n",
    "keys = filter(lambda key: isinstance(flowInfo[key],list), flowInfo.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.81 ms, sys: 3.3 ms, total: 9.11 ms\n",
      "Wall time: 18.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.option(\"multiline\",\"true\").json(\"response_1600883647838.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+---+--------------------+---------------+\n",
      "|flowConnection|       flowParameter|            flowTask|        flowVariable| id|          taskOutput|        version|\n",
      "+--------------+--------------------+--------------------+--------------------+---+--------------------+---------------+\n",
      "|            []|[[global, exprexi...|[[, [[[{taskOutpu...|[[String, -, 1, f...|  1|[[1, messageSucce...|[0, 1, 1, 0, 0]|\n",
      "+--------------+--------------------+--------------------+--------------------+---+--------------------+---------------+\n",
      "\n",
      "CPU times: user 3.97 ms, sys: 2.2 ms, total: 6.17 ms\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=BooleanType())\n",
    "def get_column_with_list(colFirstValue):\n",
    "    return isinstance(colFirstValue, list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# https://stackoverflow.com/questions/46813283/select-columns-in-pyspark-dataframe\n",
    "df_linea89 = df.select(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_linea89.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##linea 93\n",
    "## https://docs.databricks.com/spark/latest/spark-sql/udf-python.html\n",
    "\n",
    "@F.udf(\"string\")\n",
    "def getName(column_name,listOfItem):\n",
    "    if len(cte_PRIMARY_KEYS[column_name]) > 1:\n",
    "        pk = \"_\".join([listOfItem[i] for i in range(len(cte_PRIMARY_KEYS[column_name]))])\n",
    "    else:\n",
    "        pk = listOfItem[0]\n",
    "    return pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# https://stackoverflow.com/questions/34037889/apply-same-function-to-all-fields-of-spark-dataframe-row\n",
    "# https://stackoverflow.com/questions/36584812/pyspark-row-wise-function-composition\n",
    "# df_linea101_pk = df_linea89.select(*(getName(c.name,F.col(c)) for c in df_linea89.columns))\n",
    "df_linea101_pk = df_linea89.select(get_name(struct([x.name,df_linea89[x], for x in df_linea89.columns])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## la linea 106 y la 101 hacen lo mismo\n",
    "# df_linea89.withColumn(\"null_count\", count_empty_columns(struct([df[x] for x in df_linea89.columns])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# https://stackoverflow.com/questions/44067861/pyspark-add-a-new-column-with-a-tuple-created-from-columns\n",
    "df_linea109_namedTuples = df_linea89.withColumn(\"namedTuples\",\\\n",
    "                                                struct([get_name(x.name,df_linea89[x])],\\\n",
    "                                                        x.name for x in df_linea89.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# https://stackoverflow.com/questions/47682927/how-to-create-a-udf-in-pyspark-which-returns-an-array-of-strings\n",
    "get_values_in_ = udf(my_udf, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "df_linea109_namedTuplesPredefined.withColumn(\"namedTuplesPredefined\", x for x in df_linea89.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
